%
% File acl2019.tex
%
%% Based on the style files for ACL 2018, NAACL 2018/19, which were
%% Based on the style files for ACL-2015, with some improvements
%%  taken from the NAACL-2016 style
%% Based on the style files for ACL-2014, which were, in turn,
%% based on ACL-2013, ACL-2012, ACL-2011, ACL-2010, ACL-IJCNLP-2009,
%% EACL-2009, IJCNLP-2008...
%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt,a4paper]{article}
\usepackage[hyperref]{acl2019}
\usepackage{times}
\usepackage{latexsym}

\usepackage{url}
\usepackage{graphicx}

\aclfinalcopy % Uncomment this line for the final submission
\def\aclpaperid{1234} %  Enter the acl Paper ID here

%\setlength\titlebox{5cm}
% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.

\newcommand\BibTeX{B\textsc{ib}\TeX}

\title{Understanding Scanned Receipts}

\author{Eric Melz \\
  300 S Reeves Dr. \\
  Beverly Hills, CA 90212 \\
  \texttt{eric@emelz.com}
}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Tasking machines with understanding receipts can have important
applications such as enabling detailed analytics on purchases,
enforcing expense policies, and inferring patterns of purchase
behavior on large collections of receipts.  In this paper, we focus on
the task of Named Entity Linking (NEL) of 
scanned receipt line items.  Specifically, the task entails
associating shorthand text from OCR’d receipts with a knowledge base
(KB) of grocery products.  For example, the scanned item ``STO BABY
SPINACH'' should be linked to the catalog item labeled ``Simple Truth
Organic\texttrademark Baby Spinach''.  Experiments that employ a variety of
Information Retrieval techniques in combination with statistical
phrase detection shows promise for effective understanding of scanned
receipt data.

\end{abstract}

\section{Introduction}

Tasking machines with understanding receipts can have important
applications such as enabling detailed analytics on purchases,
enforcing expense policies, and inferring patterns of purchase
behavior on large collections of receipts.  In this paper, we focus on
the task of Named Entity Linking~\cite{Hachey:2012} of
scanned receipt line items.  Specifically, the task entails
associating shorthand text from OCR’d receipts with a knowledge base
(KB) of grocery products.  For example, the scanned item ``STO BABY
SPINACH'' should be linked to the catalog item labeled ``Simple Truth
Organic\texttrademark Baby Spinach''.  

\section{Related Work}

A literature review reveals virtually no published work in this
specific domain.  While there is a body of work researching text
extraction from scanned receipts (e.g.~\citealp{Huang:2019}), the work is primarily
focused on Named Entity Recognition (NER) instead of Named Entity
Linking (NEL).  That is, systems are considered successful if they can
identify text items such as store locations, totals, etc, but they are
not evaluated with respect to the interpretation of the extracted
text.


Although no papers exist on linking scanned entities, there is
literature in other areas that appear potentially relevant to the
subject task.  This includes work on general-purpose techniques for
building abbreviation dictionaries, acquisition of medical
abbreviations (e.g., ``COPD'' $\rightarrow$ ``Chronic Obstructive Pulmonary
Disorder''), and normalization of social media content (e.g., ``ur
coooool'' $\rightarrow$ ``you are cool'').  The follow sections
summarize a few papers in these areas.

\subsection{Language Independent Acquisition of Abbreviations}
~\cite{DBLP:journals/corr/abs-1709-08074} describe a
language-independent technique for acquiring abbreviations and their
expansions, by exploiting Wikipedia redirect and disambiguation pages.
They begin by motivating the acquisition of abbreviations, noting that
the explosion of social media has made the need for abbreviations
increasingly important.  They also note that a token such as ``ACE''
could have multiple expansions, including ``accumulated cyclone
energy'' and ``American Council on Education'' in addition to the word
``ace'' (as in ``Ace of spades'').

The authors present related work, noting that most of the previous
work for abbreviation detection and expansion extraction has been in
the domain of English biomedical text.  A common strategy is to
identify occurrences where an abbreviation is explicitly paired with
its expansion for example through a pattern involving a parenthetical
such as {\em \verb|<short form>  (<long form>)|} or {\em
  \verb|<long form> (<short form>)|}.
Other approaches consider the contexts of short form and long form
occurrences, pairing short forms with long forms according to their
distributional similarity by measuring the cosine of their context
vectors.  Another approach uses supervised learning, considering
features such as string similarity and other characteristics of the
short and long forms.

The author’s work is based on previous work by~\cite{JACQUET14.468}
who describe a technique for mining abbreviations by making use of
Wikipedia redirection pages.  The authors observe that, due to the use
of only redirect pages for the gold standard annotation, a shortcoming
of the prior work is that each abbreviation only has a single
expansion even though multiple different expansions are possible for
some of the abbreviations.  To remedy this shortcoming, the authors
propose mining disambiguation pages in addition to redirect pages to
gather multiple possible long-form expansions.

The authors mine redirect and disambiguation pages for abbreviations,
while applying several rules such as (a) Short forms are restricted to
ten characters or less, (b) At least half of the short-form characters
must be upper case, and (c) The long-form must be at least twice as
long as the short form, with at least two tokens.  They generate
candidate expansions and then score the expansions.  Scoring occurs by
computing features for synonym similarity, topic similarity, and
surface similarity.  Synonym similarity means that one term can be
replaced with another while preserving the meaning of the sentence and
is assessed using word embeddings using word2vec~\cite{NIPS2013_5021}.
Topical relatedness means that two terms occur in the same sorts of
documents, and is assessed using Latent Semantic
Analysis~\cite{deerwester-indexing-1990}.  Surface similarity is the
overlap in the surface forms of the terms by computing the best
possible alignment between a short form and a long form.   The three
similarity scores are combined using a logistic regression model.

The authors compare their system with a previous system developed
by~\cite{SchwartzH03} that extracts abbreviations using parentheses
based patterns.  The metric used to compare systems is Area Under the
Precision/Recall curve.  Without the scoring extensions, the 2 systems
are comparable: the Schwartz and Hearst system has an AUC of 0.359 and
the Candidate System has an AUC of 0.324.  However, by adding the
alignment and embedding scoring extensions, the Candidate System’s
performance improves to an AUC of 0.480. 

\subsection{Clinical Abbreviation Expansion}
~\cite{liu-etal-2015-exploiting} describe a system for identifying clinical abbreviation
expansions.  They note that abbreviations are heavily used in medical
literature and documentation.  In notes written by physicians, high
workloads and time pressure intensify the need for using
abbreviations.  This is especially true within intensive care
medicine, where it’s crucial that information is expressed in the most
time efficient manner to provide time-sensitive care to critically ill
patients.  Within the arena of medical research, abbreviation
expansion using NLP can enable knowledge discovery and has the
potential to improve quality of care.

The author's system works as follows.  Word embeddings are trained
using word2vec~\cite{NIPS2013_5021}.  The material used to train embeddings consists of
medical  texts such as articles, journals, and books, in addition to
hand-written Intensive Care notes.  To generate expansions for
abbreviations in the hand-written notes, abbreviations are extracted
from the notes, and then matched against a domain-specific
abbreviation knowledge base.  From this list of expansions, 
embedding vectors are retrieved for the abbreviation and candidate
expansion.  A similarity score is computed for each (abbreviation,
expansion) pair, producing a ranked list of candidates expansions.

To test the performance of the system, a ground-truth dataset is
produced by having physicians manually expand and normalize the
handwritten notes.  The authors compare their model against several
baselines.  For example, one baseline chooses the highest rated
candidate expansion in the domain specific knowledge base.  Comparing
accuracy of the author’s system against the baselines results in a
50\%+ increase.  For example the rating baseline has an accuracy of 21\%
and the authors system has an accuracy of 83\%.

\subsection{Social Media Text Normalization}
~\cite{Lourentzo:2019} present a Sequence to Sequence (Seq2Seq) model
for normalizing social media text.  They observe that social media
texts have an enormous amount of variation, and that text
normalization systems that rely on surface or phonetic representations
may be ill-equipped to handle such variability.  To rectify this
situation, they propose a hybrid word-character Seq2Seq model with
attention.  This type of model has been successfully applied to tasks
such as machine translation, and has promise for text normalization.

The authors frame the task of text normalization as mapping an
out-of-vocabulary (OOV) non-standard word to an in-vocabulary (IV)
standard word that preserves the meaning of the sentence.  The non
standard forms in user generated content include misspellings
(defenitely $\rightarrow$ definitely), phonetic substitutions (2morrow $\rightarrow$
tomorrow), shortening (convo $\rightarrow$ conversation), acronyms (idk $\rightarrow$ i don’t
know), slang (``low key'', ``woke''), emphasis (coooool $\rightarrow$ cool), and
punctuation  
(doesnt $\rightarrow$ doesn't).

The authors note that lexicon-based approaches are not able to handle
social media text properly.  String similarity, such as edit distance,
does not work on non-standard words where the number of edits is
large, for example abbreviation.   Additionally, systems that rely on
candidate generation and scoring are limited in that they are not able
to handle multiple normalization errors at once, e.g., spelling errors
on an acronym.  The authors suggest that using end-to-end neural
models, particularly Seq2Seq models can deal with these shortcomings.

The authors train a bidirectional word-based Seq2Seq model to
translate unnormalized text to normalized texts.  OOV words are
trained using a character-based Seq2Seq model.  The dataset is
enhanced by synthetically generating negative examples based on common
normalization transformations.    The network is trained on source
sequences and target sequences.  An example source is ``got exo to
share, u interested?  Concert in hk !'', with a corresponding target of
``got extra to share, are you interested?  Concert in hong kong !''.

The authors present results for several variations of the model,
including a word-level Seq2Seq model and the hybrid word-char Seq2Seq
model.  The best score is an F\textsubscript{1} score of 83.94 on the
hybrid word-char Seq2Seq model.

\section{Data}

For this task, we need a dataset which includes scanned receipt
product mentions (e.g, ``BRHD CHEESE'') and the corresponding product
entities (e.g.,  ``Boar's Head Monterey Jack with Jalapeno Pre-Sliced
Cheese'').  A brief web search revealed that no such publicly available
dataset exists.  To obtain a dataset, we built our own by scraping a
grocery store website that contains purchase data.  Specifically, we
use our personal loyalty account with Ralph's (a subsidiary of
Kroger) to obtain representations of scanned receipts along with
corresponding web pages that contain fully-resolved entities.   
As an example, Figure \ref{fig:scanned} shows an instance of a receipt. 


\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{raw_receipt}
    \caption{Scanned Receipt}
    \label{fig:scanned}
\end{figure}

Figure \ref{fig:web} shows part of the corresponding web page which contains linked representations of the purchased items.

\begin{figure}[h]
  \centering
\includegraphics[width=0.5\textwidth]{web_receipt}
\caption{Web Receipt}
\label{fig:web}
\end{figure}

We scrape both the text content of the raw receipts and the
user-friendly web rendering, then join the raw receipt data with the
corresponding web data.  This produces a JSON structure per receipt.
A sample of the JSON is shown in Figure \ref{fig:json}.  The ``raw''
field represents the product mention, and the ``web'' field represents
the label associated with the entity.  The ``id'' field is scraped from
the web page and can be used as a succinct identifier for the entity. 

\begin{figure}[h]
  \centering
\includegraphics[width=0.5\textwidth]{json}
\caption{JSON representation of joined ``raw'' and ``web'' data}
\label{fig:json}
\end{figure}

The dataset consists of 65 scraped receipts, producing 711 non-unique line
line items, and 296 unique line items.


\section{Methodology}

Blah blah blah.

\section{Experiments}

Blah blah blah.

\subsection{Baseline}

Blah blah blah

\subsection{Wildcards}

Blah blah blah

\subsection{Mashed Wildcards}

Blah blah blah

\subsection{Phrases}

Blah blah blah

\subsection{Fuzzy Phrases}

Blah blah blah

\section{Results}

blah

\section{Future Work}

blah

\section{Conclusion}

blah

\bibliography{receipt_nlu}
\bibliographystyle{acl_natbib}

\appendix

\section{Appendices}
\label{sec:appendix}

blah 

\section{Supplemental Material}
\label{sec:supplemental}
blah blah blah

\end{document}
